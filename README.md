# Algorithms-Data_Structures

<h1>Algorithms</h1>

1) Bubble Sort - Bubble sort is a simple sorting algorithm that works by repeatedly swapping adjacent elements if they are in the wrong order. It is called bubble sort because the smaller elements "bubble" to the top of the list, while the larger elements "sink" to the bottom. The algorithm proceeds by iterating through the list of elements, comparing each pair of adjacent elements, and swapping them if they are in the wrong order. This process is repeated until no more swaps are necessary, indicating that the list is sorted. Although bubble sort is simple to understand and implement, it has a worst-case time complexity of O(n^2), making it relatively inefficient for large lists. For this reason, it is usually only used for small lists or as an educational example of a sorting algorithm.

2) Insertion Sort - Insertion sort is a simple sorting algorithm that works by repeatedly taking an unsorted element and inserting it into the correct position in a sorted sub-list. It is particularly efficient for small data sets and nearly sorted lists. The algorithm proceeds by starting with a single element (the first element) and iterating through the list, comparing each subsequent element with the elements in the sorted sub-list and inserting it into the correct position. The sorted sub-list grows with each iteration until all elements have been inserted and the list is fully sorted. Insertion sort has an average and worst-case time complexity of O(n^2), making it relatively inefficient for large data sets. However, it has the advantage of being simple to understand and implement, and it performs well on small or nearly sorted lists. Additionally, insertion sort is often used as part of more complex sorting algorithms, such as merge sort or quicksort.

3) Merge Sort - Merge sort is a divide-and-conquer sorting algorithm that works by recursively dividing the input list into two halves, sorting each half, and then merging the sorted halves back together. It is a stable, comparison-based sorting algorithm with a worst-case time complexity of O(n log n). The algorithm proceeds by dividing the input list into two halves, recursively sorting each half, and then merging the sorted halves back together. The merging process involves comparing the first elements of each sorted sub-list and appending the smaller one to the output list. This process continues until one of the sub-lists is empty, at which point the remaining elements in the other sub-list are appended to the output list. Merge sort has the advantage of being a stable sorting algorithm, which means that elements with the same value retain their original order in the sorted list. Additionally, merge sort is well-suited to sorting linked lists, as it only requires changing pointers to merge the sub-lists, rather than copying elements as in an array-based implementation. However, merge sort does require additional memory for the merging process, and its recursive nature can lead to stack overflow on very large lists.

4) Quick Sort - Quick sort is a divide-and-conquer sorting algorithm that works by partitioning an input list into two sub-lists, sorting each sub-list independently, and then combining the sorted sub-lists back together. It is a highly efficient sorting algorithm, with an average time complexity of O(n log n) and a worst-case time complexity of O(n^2). The algorithm proceeds by selecting a pivot element from the input list and partitioning the list into two sub-lists, one containing elements smaller than the pivot and one containing elements larger than the pivot. The pivot element is then moved to its final position in the sorted list, and the two sub-lists are recursively sorted using the same process. Quick sort has the advantage of being an in-place sorting algorithm, meaning that it can sort the input list without requiring additional memory beyond the list itself. Additionally, quick sort is often faster than other comparison-based sorting algorithms for large data sets, due to its efficient partitioning scheme. However, quick sort can have poor performance on nearly sorted or already sorted lists, and its worst-case time complexity can be problematic for certain types of input. Various modifications to the basic algorithm, such as randomized pivot selection or hybridization with insertion sort, can mitigate these issues.

5) Selection Sort - Selection sort is a simple sorting algorithm that works by repeatedly finding the smallest element in an unsorted portion of the input list and swapping it with the first unsorted element. It has a worst-case time complexity of O(n^2), making it relatively inefficient for large data sets. The algorithm proceeds by iterating through the input list and selecting the smallest element in the unsorted portion of the list. This element is then swapped with the first unsorted element, effectively adding it to the sorted portion of the list. This process continues until the entire list is sorted. Selection sort has the advantage of being simple to implement and requiring only a small amount of additional memory for swapping elements. However, its time complexity makes it unsuitable for large data sets or real-time applications. Additionally, selection sort is not a stable sorting algorithm, meaning that the relative order of equal elements may change in the sorted list.


<h1>Data Structures</h1>

1) Binary Search Tree - A binary search tree (BST) is a data structure that organizes elements in a tree-like structure, where each node has at most two children and each node's left subtree contains only elements less than the node's value, while the right subtree contains only elements greater than the node's value. This property allows for efficient search, insertion, and deletion of elements. The structure of a BST is such that the root node represents the median element of the entire tree, with all elements less than the root node in the left subtree and all elements greater in the right subtree. This property is maintained for all nodes in the tree, making it possible to efficiently search for an element by recursively traversing the left or right subtrees based on the element's value. BSTs have several advantages over other data structures for searching and sorting, including a worst-case time complexity of O(log n) for search, insertion, and deletion operations (assuming a balanced tree), and the ability to maintain a sorted list of elements in a dynamic data structure. However, if the tree is unbalanced, the worst-case time complexity can be as bad as O(n), making it important to balance the tree periodically to maintain efficiency. Additionally, some operations, such as finding the minimum or maximum element, can be slower than in other data structures such as heaps.

2) Double Linked List - A doubly linked list is a data structure that consists of a sequence of elements, each of which contains a value and pointers to both the previous and next elements in the list. This allows for efficient traversal of the list in both directions, as well as insertion and deletion of elements at any position in the list. The structure of a doubly linked list is such that each element contains a reference to the previous and next elements in the list. This allows for efficient traversal of the list in either direction by following the appropriate pointers. Additionally, inserting or deleting an element in the middle of the list only requires updating the pointers of the adjacent elements, rather than the entire list. Doubly linked lists have several advantages over other data structures, including the ability to efficiently insert or delete elements at any position in the list and the ability to efficiently traverse the list in both directions. However, they require additional memory to store the extra pointers, and the pointers themselves can be a source of bugs if not properly managed. Additionally, doubly linked lists may not be as efficient as other data structures, such as arrays or singly linked lists, for certain operations such as indexing or sequential traversal.

3) Graph - A graph is a data structure that represents a set of objects, called vertices or nodes, and the connections between them, called edges. Graphs can be used to model a wide range of relationships, from social networks to physical systems, and are a fundamental tool in computer science and other fields. In a graph, vertices are typically represented as points or circles, and edges are represented as lines or arrows connecting pairs of vertices. The edges can be directed or undirected, meaning that they either point in a specific direction or allow for movement in either direction. The connections between vertices can also be weighted, meaning that each edge has a numerical value that represents a cost, distance, or other attribute.<br>
Graphs can be classified according to their structure and properties. Some common types of graphs include:<br>
a) Directed graphs (digraphs), where edges have a direction<br>
b) Undirected graphs, where edges do not have a direction<br>
c) Weighted graphs, where edges have a numerical weight<br>
d) Bipartite graphs, where vertices can be divided into two disjoint sets, with edges only connecting vertices from different sets<br>
e) Complete graphs, where every pair of vertices is connected by an edge<br>
f) Sparse graphs, where only a small fraction of possible edges are present<br>
g) Dense graphs, where a large fraction of possible edges are present<br>
Graphs are a powerful tool for modeling complex systems and can be used for a variety of applications, such as route planning, social network analysis, and data visualization. However, the complexity of graph algorithms can make them computationally expensive for very large graphs, and the interpretation of graph structures and properties can be challenging.

4) Hash Table - A hash table is a data structure that allows for efficient insertion, deletion, and retrieval of data using a key-value pair system. In a hash table, the key is used to compute a hash code, which is then used to look up the corresponding value in an array. The hash function takes a key as input and returns a unique integer value, which is used as an index to store the corresponding value in the array. Ideally, the hash function produces a unique index for each key, but collisions can occur when two or more keys produce the same hash code. To resolve collisions, the hash table uses a collision resolution strategy, such as chaining or open addressing. In chaining, each element of the hash table is a linked list, and collisions are resolved by adding elements with the same hash code to the linked list at the corresponding index. In open addressing, collisions are resolved by searching for an empty slot in the hash table and storing the element there. Hash tables have several advantages over other data structures, such as arrays or linked lists, for certain operations, including O(1) average-case time complexity for insertion, deletion, and retrieval of elements, and the ability to store a large amount of data in a compact space. However, hash tables may have slower worst-case time complexity for certain operations if collisions occur frequently, and the choice of hash function and collision resolution strategy can have a significant impact on performance.

5) Queue - A queue is a linear data structure that allows for the insertion of elements at the rear and removal of elements from the front. It follows the First-In-First-Out (FIFO) principle, which means that the first element added to the queue will be the first one to be removed. A queue typically has two main operations: enqueue (adding an element to the rear of the queue) and dequeue (removing an element from the front of the queue). Other operations, such as peek (viewing the front element without removing it) and size (determining the number of elements in the queue), are also commonly supported. Queues can be implemented using an array, a linked list, or other data structures. In an array implementation, a circular buffer is often used to avoid the need for shifting elements when dequeueing. In a linked list implementation, each node contains a value and a pointer to the next node in the queue. Queues are widely used in computer science and everyday life for a variety of applications, such as task scheduling, message passing, and buffer management. They are also used in conjunction with other data structures, such as stacks, to implement more complex algorithms and data structures.

6) Singly Linked List - A singly linked list is a linear data structure consisting of a sequence of nodes, where each node contains a value and a reference (pointer) to the next node in the list. The first node in the list is called the head, and the last node is called the tail. If the tail node has a null reference, the list is said to be singly linked.Singly linked lists are used to implement dynamic data structures, such as stacks, queues, and hash tables. They allow for efficient insertion and removal of elements at the beginning or end of the list, but accessing elements in the middle of the list can be slow, as it requires traversing the list from the beginning. <br>Common operations on a singly linked list include:<br>
a) Insertion: adding a new node to the beginning or end of the list<br>
b) Deletion: removing a node from the beginning or end of the list<br>
c) Traversal: visiting each node in the list to perform some operation on its value<br>
d) Searching: finding the node containing a specific value<br>
e) Concatenation: combining two lists into a single list<br>
Singly linked lists have several advantages over other data structures, such as arrays or doubly linked lists, for certain operations, including O(1) time complexity for insertion and deletion at the beginning of the list, and the ability to efficiently grow and shrink the list. However, accessing elements in the middle of the list can be slow, and they require more memory overhead than arrays for storing the references to the next node.
